WARNING:backend.core.model_loader:Model file not found: models/deepfake_detector.pt
INFO:backend.core.model_loader:Using mock model for testing
WARNING:backend.core.model_loader:\u26a0\ufe0f  Using MOCK MODEL - predictions are random!
WARNING:backend.core.model_loader:   Place a real TorchScript model at the configured MODEL_PATH
INFO:httpx:HTTP Request: GET http://testserver/metrics "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET http://testserver/metrics "HTTP/1.1 200 OK"
INFO:backend.core.risk_engine:RiskEngine initialized: window_size=10, thresholds=[LOW<0.3, MEDIUM<0.7, HIGH\u22650.7], smoothing_factor=0.3
INFO:backend.core.rate_limiter:RateLimiter initialized: max_rate=10 chunks/sec
INFO:backend.api.websocket_handler:Received 32000 bytes
INFO:backend.api.websocket_handler:Converted to numpy array: 16000 samples
INFO:backend.api.websocket_handler:Applied overlap: new length = 16000 samples
INFO:backend.core.feature_extractor:Feature extraction complete: shape=(128, 32), duration=1.00s, latency=14388.82ms
INFO:backend.core.inference_engine:Inference complete: probability=0.1304, latency=27.18ms
INFO:backend.core.risk_engine:Risk updated: raw_prob=0.1304, smoothed_prob=0.1304, rolling_risk=0.1304, level=LOW, buffer_size=1/10
INFO:httpx:HTTP Request: GET http://testserver/metrics "HTTP/1.1 200 OK"
INFO:backend.api.websocket_handler:Client disconnected normally
INFO:httpx:HTTP Request: GET http://testserver/metrics "HTTP/1.1 200 OK"

Metrics Response Structure: {'active_connections': 0, 'total_chunks_processed': 0, 'total_inference_calls': 0, 'total_errors': 0, 'average_latency_ms': 0.0, 'uptime_seconds': 0.1, 'chunks_per_second': 0.0, 'model_mode': 'mock'}
Client connected. Active connections: 1
Client disconnected. Active connections: 0
Final Metrics: {'active_connections': 0, 'total_chunks_processed': 1, 'total_inference_calls': 1, 'total_errors': 0, 'average_latency_ms': 14420.05, 'uptime_seconds': 14.94, 'chunks_per_second': 0.07, 'model_mode': 'mock'}
