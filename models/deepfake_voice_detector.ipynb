{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ™ï¸ Deepfake Voice Detection System\n",
    "### In-the-Wild Dataset â€” CNN + MFCC Pipeline\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Data Loading & MFCC Extraction |\n",
    "| 2 | Data Preprocessing & Splitting |\n",
    "| 3 | CNN Model Definition |\n",
    "| 4 | Model Training |\n",
    "| 5 | Model Testing & Metrics |\n",
    "| 6 | Verification Conditions |\n",
    "| 7 | Conditional Model Saving |\n",
    "| 8 | Prediction Function |\n",
    "| 9 | Final Testing (5 real + 5 fake) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¦ Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install missing dependencies (run once) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Uncomment the line below if librosa is not installed:\n",
    "# !pip install librosa tqdm scikit-learn\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Notebook lives inside: release_in_the_wild/\n",
    "# Audio + meta.csv live in the nested: release_in_the_wild/release_in_the_wild/\n",
    "_NB_DIR     = os.getcwd()                              # current working directory\n",
    "DATASET_DIR = os.path.join(_NB_DIR, \"release_in_the_wild\")\n",
    "\n",
    "# Fallback: if the nested folder doesn't exist, look in the current dir\n",
    "if not os.path.isdir(DATASET_DIR):\n",
    "    DATASET_DIR = _NB_DIR\n",
    "\n",
    "META_CSV    = os.path.join(DATASET_DIR, \"meta.csv\")\n",
    "MODEL_PATH  = os.path.join(_NB_DIR, \"deepfake_voice_detector.pth\")\n",
    "\n",
    "# â”€â”€ Hyper-parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "N_MFCC        = 40          # number of MFCC coefficients\n",
    "MAX_LEN       = 174         # fixed time-frame length (~4 s @ 22 050 Hz)\n",
    "SAMPLE_RATE   = 22_050      # librosa default sample rate\n",
    "EPOCHS        = 20\n",
    "BATCH_SIZE    = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "SEED          = 42\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Dataset dir : {DATASET_DIR}\")\n",
    "print(f\"meta.csv    : {META_CSV}  â†’  exists={os.path.isfile(META_CSV)}\")\n",
    "print(f\"Device      : {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 â€” Data Loading & MFCC Extraction\n",
    "\n",
    "- Load each `.wav` with **librosa** at 22 050 Hz\n",
    "- Extract **40 MFCC coefficients** per frame\n",
    "- Pad (zeros) or trim to exactly **174 frames** â†’ fixed shape `(40, 174)`\n",
    "- Label mapping: `bona-fide` â†’ **0 (Real)**, `spoof` â†’ **1 (Fake)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path: str, n_mfcc: int = N_MFCC, max_len: int = MAX_LEN):\n",
    "    \"\"\"\n",
    "    Load a .wav file with librosa, compute MFCC, pad/trim to fixed length.\n",
    "    Returns np.ndarray of shape (n_mfcc, max_len), or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
    "        mfcc  = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # (40, T)\n",
    "\n",
    "        # Pad or trim along the time axis\n",
    "        if mfcc.shape[1] < max_len:\n",
    "            pad_width = max_len - mfcc.shape[1]\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_len]\n",
    "\n",
    "        return mfcc  # (40, 174)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not load {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Read meta.csv, extract MFCC for every audio file, assign numeric labels.\n",
    "    Returns: X (N,40,174), y (N,), valid_files list\n",
    "    \"\"\"\n",
    "    print(\"[STEP 1] Loading dataset â€¦\")\n",
    "    df = pd.read_csv(META_CSV)\n",
    "\n",
    "    label_map = {\"bona-fide\": 0, \"spoof\": 1}\n",
    "    features, labels, valid_files = [], [], []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting MFCCs\"):\n",
    "        audio_path = os.path.join(DATASET_DIR, row[\"file\"])\n",
    "        if not os.path.isfile(audio_path):\n",
    "            continue\n",
    "\n",
    "        mfcc = extract_mfcc(audio_path)\n",
    "        if mfcc is None:\n",
    "            continue\n",
    "\n",
    "        features.append(mfcc)\n",
    "        labels.append(label_map[row[\"label\"]])\n",
    "        valid_files.append(row[\"file\"])\n",
    "\n",
    "    X = np.array(features, dtype=np.float32)  # (N, 40, 174)\n",
    "    y = np.array(labels,   dtype=np.int64)     # (N,)\n",
    "\n",
    "    print(f\"Loaded {len(X)} samples  (real={np.sum(y==0)}, fake={np.sum(y==1)})\")\n",
    "    return X, y, valid_files\n",
    "\n",
    "\n",
    "# Run data loading (takes ~14 min on CPU for ~31 780 files)\n",
    "X, y, valid_files = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 â€” Data Preprocessing & Splitting\n",
    "\n",
    "- Flatten + `StandardScaler` â†’ reshape to `(N, 1, 40, 174)` (channel dim for CNN)\n",
    "- Stratified split: **70% train / 15% val / 15% test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Normalize with StandardScaler and split 70 / 15 / 15 (stratified).\n",
    "    Returns: X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "    \"\"\"\n",
    "    print(\"[STEP 2] Preprocessing â€¦\")\n",
    "    N, H, W = X.shape\n",
    "\n",
    "    # Normalize\n",
    "    X_flat = X.reshape(N, -1)\n",
    "    scaler  = StandardScaler()\n",
    "    X_norm  = scaler.fit_transform(X_flat).reshape(N, 1, H, W)  # add channel dim\n",
    "\n",
    "    # 70 / 30  â†’  then 30 split 50/50  =  15 / 15\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "        X_norm, y, test_size=0.30, random_state=SEED, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "\n",
    "def to_dataloader(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool):\n",
    "    \"\"\"Convert numpy arrays to a PyTorch DataLoader.\"\"\"\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.long)\n",
    "    return DataLoader(TensorDataset(X_t, y_t), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler = preprocess(X, y)\n",
    "\n",
    "train_loader = to_dataloader(X_train, y_train, BATCH_SIZE, shuffle=True)\n",
    "val_loader   = to_dataloader(X_val,   y_val,   BATCH_SIZE, shuffle=False)\n",
    "test_loader  = to_dataloader(X_test,  y_test,  BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 â€” CNN Model Definition\n",
    "\n",
    "```\n",
    "Input: (batch, 1, 40, 174)\n",
    "  â†’ Conv2d(1â†’32)+BN+ReLU+MaxPool2d  â†’  (32, 20, 87)\n",
    "  â†’ Conv2d(32â†’64)+BN+ReLU+MaxPool2d â†’  (64, 10, 43)\n",
    "  â†’ Conv2d(64â†’128)+BN+ReLU+MaxPool2d â†’ (128, 5, 21)\n",
    "  â†’ Flatten â†’ 13,440\n",
    "  â†’ FC(13440â†’256)+Dropout(0.4)\n",
    "  â†’ FC(256â†’64)+Dropout(0.4)\n",
    "  â†’ FC(64â†’2)   â† 2 output neurons\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for deepfake voice detection.\n",
    "    Input  : (batch, 1, 40, 174)  â€” 1-channel MFCC spectrogram\n",
    "    Output : (batch, 2)           â€” logits for [real, fake]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DeepfakeCNN, self).__init__()\n",
    "\n",
    "        # Block 1: Conv â†’ BN â†’ ReLU â†’ MaxPool\n",
    "        self.conv1 = nn.Conv2d(1,   32, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)      # 40Ã—174 â†’ 20Ã—87\n",
    "\n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(32,  64, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)      # 20Ã—87  â†’ 10Ã—43\n",
    "\n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)      # 10Ã—43  â†’ 5Ã—21\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 5 * 21, 256)   # 13 440 â†’ 256\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64,  2)               # 2 output neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)               # flatten\n",
    "        x = self.drop(self.relu(self.fc1(x)))\n",
    "        x = self.drop(self.relu(self.fc2(x)))\n",
    "        return self.fc3(x)                       # raw logits\n",
    "\n",
    "\n",
    "print(\"[STEP 3] Building CNN model â€¦\")\n",
    "model = DeepfakeCNN().to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 â€” Model Training\n",
    "\n",
    "- **Optimizer**: Adam (lr = 1e-3)\n",
    "- **Loss**: CrossEntropyLoss\n",
    "- **Epochs**: 20 (minimum 10 required)\n",
    "- **Scheduler**: ReduceLROnPlateau (patience=3, factor=0.5)\n",
    "- Prints train loss, train acc, val loss, val acc every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Train the CNN for EPOCHS using Adam + CrossEntropyLoss.\n",
    "    Returns: final_val_loss (float), training_ok (bool)\n",
    "    \"\"\"\n",
    "    print(f\"[STEP 4] Training for {EPOCHS} epochs â€¦\\n\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=3, factor=0.5\n",
    "    )\n",
    "\n",
    "    training_ok    = True\n",
    "    final_val_loss = float(\"nan\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # â”€â”€ Training phase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss    = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss    += loss.item() * X_batch.size(0)\n",
    "            preds          = outputs.argmax(dim=1)\n",
    "            train_correct += (preds == y_batch).sum().item()\n",
    "            train_total   += X_batch.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / train_total\n",
    "        train_acc      = train_correct / train_total\n",
    "\n",
    "        # â”€â”€ Validation phase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                outputs     = model(X_batch)\n",
    "                loss        = criterion(outputs, y_batch)\n",
    "                val_loss   += loss.item() * X_batch.size(0)\n",
    "                preds       = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total   += X_batch.size(0)\n",
    "\n",
    "        avg_val_loss   = val_loss / val_total\n",
    "        val_acc        = val_correct / val_total\n",
    "        final_val_loss = avg_val_loss\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Guard against NaN loss\n",
    "        if np.isnan(avg_train_loss) or np.isnan(avg_val_loss):\n",
    "            print(f\"[ERROR] NaN loss at epoch {epoch}. Stopping.\")\n",
    "            training_ok = False\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch:>2}/{EPOCHS}]  \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}  Train Acc: {train_acc:.4f}  |  \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}  Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    return final_val_loss, training_ok\n",
    "\n",
    "\n",
    "final_val_loss, training_ok = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 â€” Model Testing & Metrics\n",
    "\n",
    "Evaluate on the held-out **test set** (15% of total data) and report:\n",
    "- âœ… Test Accuracy\n",
    "- âœ… Confusion Matrix\n",
    "- âœ… Precision\n",
    "- âœ… Recall\n",
    "- âœ… F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate on the test set.\n",
    "    Returns test_accuracy (float).\n",
    "    \"\"\"\n",
    "    print(\"[STEP 5] Evaluating on test set â€¦\")\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            preds   = model(X_batch).argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    all_preds  = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    accuracy  = np.mean(all_preds == all_labels)\n",
    "    cm        = confusion_matrix(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall    = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1        = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"\\n  Test Accuracy  : {accuracy * 100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix (rows=actual, cols=predicted):\")\n",
    "    print(f\"  {cm}\")\n",
    "    print(f\"  Precision      : {precision:.4f}\")\n",
    "    print(f\"  Recall         : {recall:.4f}\")\n",
    "    print(f\"  F1 Score       : {f1:.4f}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "test_accuracy = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Steps 6 & 7 â€” Verification + Conditional Model Saving\n",
    "\n",
    "Model is saved **only if ALL four conditions pass**:\n",
    "\n",
    "| Gate | Condition |\n",
    "|------|-----------|\n",
    "| âœ” | Test accuracy â‰¥ 85% |\n",
    "| âœ” | Training completed without errors |\n",
    "| âœ” | Validation loss is not NaN |\n",
    "| âœ” | â‰¥ 5 out of 20 random samples predicted correctly |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_and_save(model, scaler, test_accuracy, final_val_loss,\n",
    "                    training_ok, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Run all 4 verification gates. Save deepfake_voice_detector.pth only if all pass.\n",
    "    Returns: all_passed (bool)\n",
    "    \"\"\"\n",
    "    print(\"[STEP 6] Model Verification â€¦\\n\")\n",
    "\n",
    "    # Gate 1 â€“ accuracy\n",
    "    cond_acc = test_accuracy >= 0.85\n",
    "    print(f\"  [{'PASS' if cond_acc else 'FAIL'}] Test accuracy >= 85%  \"\n",
    "          f\"({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "    # Gate 2 â€“ no training error\n",
    "    cond_train = training_ok\n",
    "    print(f\"  [{'PASS' if cond_train else 'FAIL'}] Training completed without errors\")\n",
    "\n",
    "    # Gate 3 â€“ val loss not NaN\n",
    "    cond_loss = not np.isnan(final_val_loss)\n",
    "    print(f\"  [{'PASS' if cond_loss else 'FAIL'}] Validation loss is not NaN  \"\n",
    "          f\"({final_val_loss:.4f})\")\n",
    "\n",
    "    # Gate 4 â€“ random sample correctness\n",
    "    model.eval()\n",
    "    random_indices = random.sample(range(len(X_test)), min(20, len(X_test)))\n",
    "    correct_count  = 0\n",
    "    with torch.no_grad():\n",
    "        for idx in random_indices:\n",
    "            x_s  = torch.tensor(X_test[idx:idx+1], dtype=torch.float32).to(DEVICE)\n",
    "            pred = model(x_s).argmax(dim=1).item()\n",
    "            if pred == y_test[idx]:\n",
    "                correct_count += 1\n",
    "    cond_samples = correct_count >= 5\n",
    "    print(f\"  [{'PASS' if cond_samples else 'FAIL'}] >= 5/20 random samples correct  \"\n",
    "          f\"({correct_count}/20 correct)\")\n",
    "\n",
    "    all_passed = cond_acc and cond_train and cond_loss and cond_samples\n",
    "\n",
    "    print()\n",
    "    if all_passed:\n",
    "        print(\"[STEP 7] All conditions passed!\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"scaler_mean\" :     scaler.mean_,\n",
    "                \"scaler_scale\":     scaler.scale_,\n",
    "                \"n_mfcc\"      :     N_MFCC,\n",
    "                \"max_len\"     :     MAX_LEN,\n",
    "                \"sample_rate\" :     SAMPLE_RATE,\n",
    "            },\n",
    "            MODEL_PATH,\n",
    "        )\n",
    "        print(\"Model verified and saved successfully.\")\n",
    "    else:\n",
    "        print(\"Model failed verification. Not saving.\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "all_passed = verify_and_save(\n",
    "    model, scaler, test_accuracy, final_val_loss,\n",
    "    training_ok, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 â€” Prediction Function\n",
    "\n",
    "`predict_audio(file_path)` â€” loads any `.wav`, extracts MFCC, runs inference, and prints:\n",
    "- `\"Real Voice\"` or `\"Fake Voice\"`\n",
    "\n",
    "The model is kept in memory after training; if called standalone it lazy-loads from `deepfake_voice_detector.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module-level references for lazy loading\n",
    "_MODEL  = None\n",
    "_SCALER = None\n",
    "\n",
    "\n",
    "def _load_model_for_inference():\n",
    "    \"\"\"Load model + scaler from disk (used when running predict_audio standalone).\"\"\"\n",
    "    checkpoint    = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    scaler        = StandardScaler()\n",
    "    scaler.mean_  = checkpoint[\"scaler_mean\"]\n",
    "    scaler.scale_ = checkpoint[\"scaler_scale\"]\n",
    "    m = DeepfakeCNN().to(DEVICE)\n",
    "    m.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    m.eval()\n",
    "    return m, scaler\n",
    "\n",
    "\n",
    "def predict_audio(file_path: str):\n",
    "    \"\"\"\n",
    "    Predict whether an audio file is a real or deepfake voice.\n",
    "\n",
    "    Args:\n",
    "        file_path : str â€” path to a .wav file\n",
    "\n",
    "    Prints:\n",
    "        \"Real Voice\"  or  \"Fake Voice\"\n",
    "\n",
    "    Returns:\n",
    "        str â€” \"Real Voice\" or \"Fake Voice\"\n",
    "    \"\"\"\n",
    "    global _MODEL, _SCALER\n",
    "\n",
    "    # Lazy-load model from disk if needed\n",
    "    if _MODEL is None or _SCALER is None:\n",
    "        if not os.path.isfile(MODEL_PATH):\n",
    "            raise FileNotFoundError(\n",
    "                f\"No saved model at {MODEL_PATH}. \"\n",
    "                \"Run the full training pipeline first.\"\n",
    "            )\n",
    "        _MODEL, _SCALER = _load_model_for_inference()\n",
    "\n",
    "    mfcc = extract_mfcc(file_path)\n",
    "    if mfcc is None:\n",
    "        print(f\"[ERROR] Could not process {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Normalize with the training scaler\n",
    "    flat   = mfcc.reshape(1, -1)\n",
    "    norm   = _SCALER.transform(flat).reshape(1, 1, N_MFCC, MAX_LEN)\n",
    "    tensor = torch.tensor(norm, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    _MODEL.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = _MODEL(tensor).argmax(dim=1).item()\n",
    "\n",
    "    label = \"Fake Voice\" if pred == 1 else \"Real Voice\"\n",
    "    print(f\"  [{os.path.basename(file_path)}]  â†’  {label}\")\n",
    "    return label\n",
    "\n",
    "\n",
    "# If training passed, wire the in-memory model so predict_audio doesn't reload from disk\n",
    "if all_passed:\n",
    "    _MODEL  = model\n",
    "    _SCALER = scaler\n",
    "\n",
    "print(\"predict_audio() is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 â€” Final Testing (5 Real + 5 Fake)\n",
    "\n",
    "Run `predict_audio()` on randomly selected samples to confirm the model generalises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(valid_files, labels):\n",
    "    \"\"\"Test predict_audio on 5 real and 5 fake audio files.\"\"\"\n",
    "    print(\"[STEP 9] Final Testing â€” 5 real + 5 fake files\\n\")\n",
    "\n",
    "    real_files = [f for f, l in zip(valid_files, labels) if l == 0]\n",
    "    fake_files = [f for f, l in zip(valid_files, labels) if l == 1]\n",
    "\n",
    "    sampled_real = random.sample(real_files, min(5, len(real_files)))\n",
    "    sampled_fake = random.sample(fake_files, min(5, len(fake_files)))\n",
    "\n",
    "    print(\">> Real voice samples:\")\n",
    "    for fname in sampled_real:\n",
    "        predict_audio(os.path.join(DATASET_DIR, fname))\n",
    "\n",
    "    print(\"\\n>> Fake voice samples:\")\n",
    "    for fname in sampled_fake:\n",
    "        predict_audio(os.path.join(DATASET_DIR, fname))\n",
    "\n",
    "\n",
    "if all_passed:\n",
    "    final_test(valid_files, y)\n",
    "else:\n",
    "    print(\"[INFO] Skipping final test â€” model did not pass verification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Standalone Prediction (Optional)\n",
    "\n",
    "After training is complete, you can call `predict_audio()` on any `.wav` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: predict on any file path\n",
    "# predict_audio(r\"C:\\path\\to\\your\\audio.wav\")\n",
    "\n",
    "# Or load from the dataset directory:\n",
    "# predict_audio(os.path.join(DATASET_DIR, \"0.wav\"))\n",
    "\n",
    "print(\"Uncomment a line above and provide a .wav path to test inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
